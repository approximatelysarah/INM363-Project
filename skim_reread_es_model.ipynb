{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A PyTorch model with Skimming, rereading, and early stopping.\n",
    "\n",
    "Use REINFORCE with baseline.\n",
    "\n",
    "Reward function:\n",
    "Use a single reward for an episode.\n",
    "If the prediction is correct, the reward is 1. Else the reward is -1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Bernoulli, Categorical\n",
    "from torchtext import datasets\n",
    "from torchtext import data\n",
    "from torchtext.data import Field, Dataset, Example\n",
    "import os\n",
    "import time\n",
    "import numpy as np \n",
    "import random\n",
    "import argparse\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device: \", device)\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "set_seed(2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>\"Fears for T N pension after talks\",\"Unions re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>\"The Race is On: Second Private Team Sets Laun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>\"Ky. Company Wins Grant to Study Peptides (AP)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>\"Prediction Unit Helps Forecast Wildfires (AP)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>\"Calif. Aims to Limit Farm-Related Smog (AP)\",...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label                                               Text\n",
       "0      3  \"Fears for T N pension after talks\",\"Unions re...\n",
       "1      4  \"The Race is On: Second Private Team Sets Laun...\n",
       "2      4  \"Ky. Company Wins Grant to Study Peptides (AP)...\n",
       "3      4  \"Prediction Unit Helps Forecast Wildfires (AP)...\n",
       "4      4  \"Calif. Aims to Limit Farm-Related Smog (AP)\",..."
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AG_news = pd.read_csv('AG_news_raw.csv', names=['Label', 'Text'], dtype={'Label':'int', 'Text':'str'}, index_col=False)\n",
    "AG_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function dict.items>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fields = {'Text':TEXT, 'Label':LABEL}\n",
    "AG_news_dict = AG_news.to_dict()\n",
    "fields.items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataFrame' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-f172c5f8c444>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAG_news\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'DataFrame' object is not callable"
     ]
    }
   ],
   "source": [
    " class DataFrameDataset(Dataset):\n",
    "     \"\"\"Class for using pandas DataFrames as a datasource\"\"\"\n",
    "     def __init__(self, examples, fields, filter_pred=None):\n",
    "         \"\"\"\n",
    "         Create a dataset from a pandas dataframe of examples and Fields\n",
    "         Arguments:\n",
    "             examples pd.DataFrame: DataFrame of examples\n",
    "             fields {str: Field}: The Fields to use in this tuple. The\n",
    "                 string is a field name, and the Field is the associated field.\n",
    "             filter_pred (callable or None): use only exanples for which\n",
    "                 filter_pred(example) is true, or use all examples if None.\n",
    "                 Default is None\n",
    "         \"\"\"\n",
    "         self.examples = examples.apply(SeriesExample.fromSeries, args=(fields,), axis=1).tolist()\n",
    "         if filter_pred is not None:\n",
    "             self.examples = filter(filter_pred, self.examples)\n",
    "         self.fields = dict(fields)\n",
    "         # Unpack field tuples\n",
    "         for n, f in list(self.fields.items()):\n",
    "             if isinstance(n, tuple):\n",
    "                 self.fields.update(zip(n, f))\n",
    "                 del self.fields[n]\n",
    "\n",
    " class SeriesExample(Example):\n",
    "     \"\"\"Class to convert a pandas Series to an Example\"\"\"\n",
    "\n",
    "     @classmethod\n",
    "     def fromSeries(cls, data, fields):\n",
    "         return cls.fromdict(data.to_dict(), fields)\n",
    "\n",
    "     @classmethod\n",
    "     def fromdict(cls, data, fields):\n",
    "         ex = cls()\n",
    "\n",
    "         for key, field in fields.items():\n",
    "            setattr(ex, key, field.preprocess(data[key]))\n",
    "\n",
    "    return ex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(sequential=True, tokenize='spacy', lower=True, fix_length=400) \n",
    "LABEL = data.LabelField(dtype=torch.float)\n",
    "\n",
    "# Split the IMDB data into training, validation and testing sets\n",
    "print('Splitting data...')\n",
    "train, test_data = datasets.IMDB.splits(TEXT, LABEL) # 25,000 training and 25,000 testing data\n",
    "train_data, valid_data = train.split(split_ratio=0.8) # split training data into 20,000 training and 5,000 vlidation sample\n",
    "\n",
    "print(\"Number of training examples: \",{len(train_data)})\n",
    "print(\"Number of validation examples: \",{len(valid_data)})\n",
    "print(\"Number of testing examples: \",{len(test_data)})\n",
    "\n",
    "MAX_VOCAB_SIZE = 25000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "splits() got multiple values for argument 'path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-14e513f2d67e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTabularDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAG_news\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: splits() got multiple values for argument 'path'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data...\n",
      "Number of training examples:  {1301}\n",
      "Number of validation examples:  {325}\n",
      "Number of testing examples:  {25000}\n",
      "Building vocabulary...\n",
      "Building iterators...\n"
     ]
    }
   ],
   "source": [
    "# Define datatypes with instructions for converting to tensors \n",
    "# sequential=True:    tokenisation applied\n",
    "# tokenize='spacy':   SpaCy tokenizer used to tokenize strings into sequential examples\n",
    "# lower=True:         convert text to lowercase\n",
    "# fix_length=400:     all examples padded to length 400\n",
    "# dtype=torch.float:  torch.dtype class that represents a batch of examples of this kind of data\n",
    "TEXT = data.Field(sequential=True, tokenize='spacy', lower=True, fix_length=400) \n",
    "LABEL = data.LabelField(dtype=torch.float)\n",
    "\n",
    "# Split the IMDB data into training, validation and testing sets\n",
    "print('Splitting data...')\n",
    "train, test_data = datasets.IMDB.splits(TEXT, LABEL) # 25,000 training and 25,000 testing data\n",
    "train_data, valid_data = train.split(split_ratio=0.8) # split training data into 20,000 training and 5,000 vlidation sample\n",
    "\n",
    "print(\"Number of training examples: \",{len(train_data)})\n",
    "print(\"Number of validation examples: \",{len(valid_data)})\n",
    "print(\"Number of testing examples: \",{len(test_data)})\n",
    "\n",
    "MAX_VOCAB_SIZE = 25000\n",
    "\n",
    "# Construct vocab objects for the text and label fields using the training datasets only\n",
    "# max_size=MAX_VOCAB_SIZE:         limit vocab size to 25,000 words\n",
    "# vectors=\"glove.6B.100d\"          produce word embeddings using glove.6B.100d (global vectors for word representation)\n",
    "# unk_init = torch.Tensor.normal_  initialize out-of-vocabulary word vectors to a random sample from the tensor\n",
    "print('Building vocabulary...')\n",
    "TEXT.build_vocab(train_data, max_size=MAX_VOCAB_SIZE, vectors=\"glove.6B.100d\", unk_init = torch.Tensor.normal_)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "# Split the datasets into batches (how many items to send to the model in each iteration). \n",
    "# Model is updated in increments according to each batch to help prevent overfitting\n",
    "BATCH_SIZE = 1  # the batch size for a dataset iterator\n",
    "print('Building iterators...')\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torchtext.datasets.imdb.IMDB object at 0x7ffa9be62898>\n"
     ]
    }
   ],
   "source": [
    "print(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build Network Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Generate Embeddings: CNN and LSTM Network\n",
    "Takes input, creates embedding, applies convolutional layer and one-layer LSTM. Outputs hidden representation (ht)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, embedding_dim, ker_size, n_filters, hidden_dim): # Initialise nn.Module \n",
    "        super().__init__()\n",
    "        \n",
    "        # Define layers\n",
    "        # 1. Embedding layer stores word embeddings as indices for later retreival\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        \n",
    "        # 2. 2D convolution layer\n",
    "            # in_channels:  1 input channel\n",
    "            # out_channels: number of channels produced by the convolution set in training \n",
    "            # kernel_size:  size of convolving kernel set in training\n",
    "        self.conv = nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(ker_size, embedding_dim))\n",
    "        \n",
    "        # 3. Single layer LSTM CNN with dropout (randomly turn off 10% of neurons to prevent co-adaptation)\n",
    "            # n_filters:  input size i.e. the number of features in the input\n",
    "            # hidden_dim: hidden size i.e. the number of features in the hidden state\n",
    "        self.lstm = nn.LSTM(input_size=n_filters, hidden_size=hidden_dim)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        \n",
    "        # 4. Run ReLU activation function over the embedding layer neurons\n",
    "        self.relu = nn.ReLU()\n",
    "   \n",
    "    def forward(self, text, h_0): # Define forward pass with input text\n",
    "        \n",
    "        # Layer 1. Apply the embedding method to the text\n",
    "        embedded = self.embedding(text)\n",
    "        #print(embeded.size())\n",
    "        \n",
    "        # Layer 2. Apply convolution, ReLU and dropout\n",
    "        conved = self.relu(self.conv(embedded.unsqueeze(1)))  # 1 * 128 * 16 * 1\n",
    "        #print(conved.size())\n",
    "        \n",
    "        batch = conved.size()[0]\n",
    "        conved = self.dropout(conved)\n",
    "        conved = conved.squeeze(3)  # 1 * 128 * 16\n",
    "        conved = torch.transpose(conved, 1, 2)  # 1 * 16 * 128\n",
    "        conved = torch.transpose(conved, 1, 0)  # 16 * 1 * 128\n",
    "        c_0 = torch.zeros([1, batch, 128]).to(device)\n",
    "        output, (hidden, cell) = self.lstm(conved, (h_0, c_0))\n",
    "        ht = hidden.squeeze(0)  # 1 * 128\n",
    "        return ht"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At every time step, the model reads one chunk which has a size of 20 words.\n",
    "\n",
    "--- input & output dimension ---\n",
    "\n",
    "Input text: 1 * 20\n",
    "\n",
    "**Embedding**\n",
    "1.Input: 1 * 20\n",
    "2.Output: 1 * 20 * 100\n",
    "\n",
    "**CNN**\n",
    "1. Input(minibatch×in_channels×iH×iW): 1 * 1 * 20 * 100\n",
    "2. Output(minibatch×out_channels×oH×oW): 1 * 128 * 16 * 1\n",
    "\n",
    "**LSTM**\n",
    "1. Inputs: input, (h_0, c_0)\n",
    "input(seq_len, batch, input_size): (16, 1 , 128)\n",
    "c_0(num_layers * num_directions, batch, hidden_size): (1 * 1, 1, 128)\n",
    "h_0(num_layers * num_directions, batch, hidden_size): (1 * 1, 1, 128)\n",
    "2. Outputs: output, (h_n, c_n)\n",
    "output:\n",
    "h_n(num_layers * num_directions, batch, hidden_size): (1 * 1, 1, 128)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Stopping Module: MLP\n",
    "Three hidden-layer MLP with 128 hidden units per layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy_S(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()    \n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.relu = nn.ReLU()\n",
    "          \n",
    "    def forward(self, ht):\n",
    "        # Input -> hidden 1\n",
    "        out = self.fc1(ht)\n",
    "        out = self.dropout(out)\n",
    "        out = self.relu(out)\n",
    "        # Hidden 1 -> hidden 2\n",
    "        out = self.fc2(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.relu(out)\n",
    "        # Hidden 2 -> hidden 3\n",
    "        out = self.fc3(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.relu(out)\n",
    "        # Hidden 3 -> output\n",
    "        out = self.fc4(out)\n",
    "        out = torch.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Policy Module (re-reading and skipping): MLP\n",
    "Three hidden-layer MLP with 128 hidden units per layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy_N(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()    \n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "         \n",
    "    def forward(self, ht):\n",
    "        out = self.fc1(ht)\n",
    "        out = self.dropout(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.fc2(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.fc3(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc4(out)\n",
    "        out = self.softmax(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Classifier: MLP\n",
    "Single-layer MLP with 128 hidden units per layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy_C(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        #self.fc = nn.Linear(input_dim, output_dim)\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "         \n",
    "    def forward(self, ht):\n",
    "        #return self.fc(ht)\n",
    "        out = self.fc1(ht)\n",
    "        out = self.dropout(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Value Network??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    '''Baseline\n",
    "    Reduce the variance.\n",
    "\n",
    "    Single-layer MLP with 128 hidden units.\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        #self.fc = nn.Linear(input_dim, output_dim)\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, ht):\n",
    "        out = self.fc1(ht)\n",
    "        out = self.dropout(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Set Parameters for Model Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up parameters\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "KER_SIZE = 5\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = 1\n",
    "CHUNCK_SIZE = 20\n",
    "MAX_K = 4  # the output dimension for step size 0, 1, 2, 3\n",
    "LABEL_DIM = 2\n",
    "N_FILTERS = 128\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "\n",
    "#gamma = args.gamma\n",
    "#alpha = args.alpha\n",
    "#learning_rate = 0.001\n",
    "\n",
    "gamma = 0.2\n",
    "alpha = 0.99\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "# the number of training epoches\n",
    "num_of_epoch = 10\n",
    "# the number of batch size for gradient descent when training\n",
    "batch_sz = 50\n",
    "\n",
    "# set up the criterion\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "# set up models\n",
    "clstm = CNN_LSTM(INPUT_DIM, EMBEDDING_DIM, KER_SIZE, N_FILTERS, HIDDEN_DIM).to(device)\n",
    "print(clstm)\n",
    "policy_s = Policy_S(HIDDEN_DIM, HIDDEN_DIM, OUTPUT_DIM).to(device)\n",
    "policy_n = Policy_N(HIDDEN_DIM, HIDDEN_DIM, MAX_K).to(device)\n",
    "policy_c = Policy_C(HIDDEN_DIM, HIDDEN_DIM, LABEL_DIM).to(device)\n",
    "value_net = ValueNetwork(HIDDEN_DIM, HIDDEN_DIM, OUTPUT_DIM).to(device)\n",
    "\n",
    "\n",
    "# set up optimiser\n",
    "params_pg = list(policy_s.parameters()) + list(policy_c.parameters()) + list(policy_n.parameters())\n",
    "optim_loss = optim.Adam(clstm.parameters(), lr=learning_rate)\n",
    "optim_policy = optim.Adam(params_pg, lr=learning_rate)\n",
    "optim_value = optim.Adam(value_net.parameters(), lr=learning_rate)\n",
    "\n",
    "# add pretrained embeddings\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "clstm.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "clstm.embedding.weight.requires_grad = True  # update the initial weights\n",
    "\n",
    "# set the default tensor type for GPU\n",
    "#torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "def finish_episode(policy_loss_sum, encoder_loss_sum, baseline_value_batch):\n",
    "    '''\n",
    "    Called when a data sample has been processed.\n",
    "    '''\n",
    "    baseline_value_sum = torch.stack(baseline_value_batch).sum()\n",
    "    policy_loss = torch.stack(policy_loss_sum).mean()\n",
    "    encoder_loss = torch.stack(encoder_loss_sum).mean()\n",
    "    objective_loss = encoder_loss - policy_loss + baseline_value_sum\n",
    "    # set gradient to zero\n",
    "    optim_loss.zero_grad()\n",
    "    optim_policy.zero_grad()\n",
    "    optim_value.zero_grad()\n",
    "    # back propagation\n",
    "    objective_loss.backward()\n",
    "    # gradient update\n",
    "    optim_loss.step()\n",
    "    optim_policy.step()\n",
    "    optim_value.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare Model Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute FLOPs(Floating point operations) of the models.\n",
    "\n",
    "cnn_cost: CNN model (which is separated from CNN_LSTM)\n",
    "s_cost: policy s (stopping module)\n",
    "c_cost: policy c (classifier)\n",
    "lstm_cost: LSTM model (which is separated from CNN_LSTM)\n",
    "cnn_whole: CNN model with whole reading(400 words).\n",
    "\n",
    "The costs are based on the size of one chunk (20 words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up parameters\n",
    "CHUNK_SIZE = 20\n",
    "BATCH_SIZE = 1\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "cnn_cost = 1024000\n",
    "s_cost = 50050\n",
    "c_cost = 16770 \n",
    "n_cost = 50310\n",
    "lstm_cost = 286720\n",
    "clstm_cost = cnn_cost + lstm_cost\n",
    "cnn_whole = 25344000\n",
    "\n",
    "\n",
    "def sample_policy_s(ht, policy_s):\n",
    "    '''\n",
    "    Draw a stopping decision from a Bernoulli distribution specified by policy s.\n",
    "    '''\n",
    "    s_prob = policy_s(ht)\n",
    "    m = Bernoulli(s_prob)\n",
    "    stop_decision = m.sample()\n",
    "    # compute the log prob\n",
    "    log_prob_s = m.log_prob(stop_decision)\n",
    "    return stop_decision, log_prob_s\n",
    "\n",
    "def sample_policy_c(output_c):\n",
    "    '''\n",
    "    Draw a label from a multinomial distribution specified by policy c.\n",
    "    '''\n",
    "    prob_c = F.softmax(output_c, dim=1)\n",
    "    m = Categorical(prob_c)\n",
    "    pred_label = m.sample()\n",
    "    log_prob_c = m.log_prob(pred_label)\n",
    "    return pred_label, log_prob_c\n",
    "\n",
    "def sample_policy_n(ht, policy_n):\n",
    "    '''\n",
    "    Draw an action from a multinomial distribution specified by policy n.\n",
    "    '''\n",
    "    action_probs = policy_n(ht)\n",
    "    m = Categorical(action_probs)\n",
    "    step = m.sample()\n",
    "    log_prob_n = m.log_prob(step)\n",
    "    return step.item(), log_prob_n\n",
    "    \n",
    "def compute_policy_value_losses(cost_ep, loss, saved_log_probs, baseline_value_ep, alpha, gamma):\n",
    "    '''compute the policy losses and value losses for the current episode\n",
    "    '''\n",
    "    # normalise cost\n",
    "    norm_cost_ep = (cost_ep - np.mean(cost_ep)) / (np.std(cost_ep) + 1e-7)\n",
    "    #print('norm_cost_ep:', norm_cost_ep)\n",
    "    reward_ep = - alpha * norm_cost_ep\n",
    "    reward_ep[-1] -= loss.item()\n",
    "    # compute discounted rewards\n",
    "    discounted_rewards = [r * gamma ** i for i, r in enumerate(reward_ep)]\n",
    "    policy_loss_ep = []\n",
    "    value_losses = []\n",
    "    for i, log_prob in enumerate(saved_log_probs):\n",
    "        # baseline_value_ep[i].item(): updating the policy loss doesn't include the gradient of baseline values\n",
    "        advantage = sum(discounted_rewards) - baseline_value_ep[i].item()\n",
    "        policy_loss_ep.append(log_prob * advantage)\n",
    "        value_losses.append((sum(discounted_rewards) - baseline_value_ep[i]) ** 2)   \n",
    "    return policy_loss_ep, value_losses\n",
    "\n",
    "\n",
    "def evaluate(clstm, policy_s, policy_n, policy_c, iterator):\n",
    "    '''\n",
    "    Evaluate a model with skimming, rereading, and early stopping\n",
    "    and compute the average FLOPs per data.\n",
    "    '''\n",
    "    # set the models in evaluation mode\n",
    "    clstm.eval()\n",
    "    policy_s.eval()\n",
    "    policy_n.eval()\n",
    "    policy_c.eval()\n",
    "    count_all = 0\n",
    "    count_correct = 0\n",
    "    start = time.time()\n",
    "    # the sum of FLOPs of the iterator set\n",
    "    flops_sum = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            label = batch.label.to(torch.long)  # for cross entropy loss, the long type is required\n",
    "            text = batch.text.view(CHUNK_SIZE, BATCH_SIZE, CHUNK_SIZE) # transform 1*400 to 20*1*20\n",
    "            curr_step = 0\n",
    "            h_0 = torch.zeros([1,1,128]).to(device)\n",
    "            count = 0\n",
    "            while curr_step < 20 and count < 5: # loop until a text can be classified or currstep is up to 20\n",
    "                count += 1\n",
    "                # pass the input through cnn-lstm and policy s\n",
    "                text_input = text[curr_step] # text_input 1*20\n",
    "                ht = clstm(text_input, h_0)  # 1 * 128\n",
    "                h_0 = ht.unsqueeze(0)  # 1 * 1 * 128, next input of lstm\n",
    "                # draw a stop decision\n",
    "                stop_decision, log_prob_s = sample_policy_s(ht, policy_s)\n",
    "                flops_sum += clstm_cost + s_cost\n",
    "                stop_decision = stop_decision.item()\n",
    "                if stop_decision == 1: # classify\n",
    "                    break\n",
    "                else:\n",
    "                    # draw an action (reread or skip)\n",
    "                    step, log_prob_n = sample_policy_n(ht, policy_n)\n",
    "                    flops_sum += n_cost\n",
    "                    curr_step += int(step)  # reread or skip\n",
    "            # draw a predicted label\n",
    "            output_c = policy_c(ht)\n",
    "            flops_sum += c_cost\n",
    "            # draw a predicted label \n",
    "            pred_label, log_prob_c = sample_policy_c(output_c)\n",
    "            if pred_label.item() == label:\n",
    "                count_correct += 1\n",
    "            count_all += 1\n",
    "    print('Evaluation time elapsed: %.2f s' % (time.time() - start))\n",
    "    avg_flop_per_sample = int(flops_sum / len(iterator))\n",
    "    print('Average FLOPs per sample: ', avg_flop_per_sample)\n",
    "    return count_all, count_correct\n",
    "\n",
    "\n",
    "def evaluate_earlystop(clstm, policy_s, policy_c, iterator):\n",
    "    '''\n",
    "    Evaluate a early stopping model with only a stopping module\n",
    "    and compute the average FLOPs per data.\n",
    "    '''\n",
    "    # set the models in evaluation mode\n",
    "    clstm.eval()\n",
    "    policy_s.eval()\n",
    "    policy_c.eval()\n",
    "    count_all = 0\n",
    "    count_correct = 0\n",
    "    start = time.time()\n",
    "    # the sum of FLOPs of the iterator set\n",
    "    flops_sum = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            label = batch.label.to(torch.long) # 64\n",
    "            text = batch.text.view(CHUNK_SIZE, BATCH_SIZE, CHUNK_SIZE) # transform 1*400 to 20*1*20\n",
    "            curr_step = 0\n",
    "            # set up the initial input for lstm\n",
    "            h_0 = torch.zeros([1,1,128]).to(device) \n",
    "            saved_log_probs = []\n",
    "            while (curr_step < 20):\n",
    "                '''\n",
    "                loop until stop decision equals 1 \n",
    "                or the whole text has been read\n",
    "                '''\n",
    "                # read a chunk\n",
    "                text_input = text[curr_step]\n",
    "                # hidden state\n",
    "                ht = clstm(text_input, h_0)  # 1 * 128\n",
    "                h_0 = ht.unsqueeze(0).cuda()  # 1 * 1 * 128, next input of lstm\n",
    "                # draw a stop decision\n",
    "                stop_decision, log_prob_s = sample_policy_s(ht, policy_s)\n",
    "                stop_decision = stop_decision.item()\n",
    "                flops_sum += clstm_cost + s_cost\n",
    "                if stop_decision == 1:\n",
    "                    break\n",
    "                else:\n",
    "                    curr_step += 1\n",
    "            # output of classifier       \n",
    "            output_c = policy_c(ht)\n",
    "            flops_sum += c_cost\n",
    "            # draw a predicted label \n",
    "            pred_label, log_prob_c = sample_policy_c(output_c)\n",
    "            if pred_label.item() == label:\n",
    "                count_correct += 1\n",
    "            count_all += 1     \n",
    "    print('Evaluation time elapsed: %.2f s' % (time.time() - start))\n",
    "    avg_flop_per_sample = int(flops_sum / len(iterator))\n",
    "    print('Average FLOPs per sample: ', avg_flop_per_sample)  \n",
    "    return count_all, count_correct\n",
    "\n",
    "\n",
    "def print_model_parm_flops(model, input):\n",
    "    '''\n",
    "    Compute FLOPs of a model.\n",
    "    '''\n",
    "    multiply_adds = False\n",
    "    list_conv=[]\n",
    "    def conv_hook(self, input, output):\n",
    "        batch_size, input_channels, input_height, input_width = input[0].size()\n",
    "        print('input size', input[0].size())\n",
    "        print('output size:', output[0].size())\n",
    "        output_channels, output_height, output_width = output[0].size()\n",
    "        kernel_ops = self.kernel_size[0] * self.kernel_size[1] * (self.in_channels / self.groups) * (2 if multiply_adds else 1)-1\n",
    "        bias_ops = 1 if self.bias is not None else 0\n",
    "        params = output_channels * (kernel_ops + bias_ops)\n",
    "        flops = batch_size * params * output_height * output_width\n",
    "        list_conv.append(flops)\n",
    "\n",
    "    list_linear=[] \n",
    "    def linear_hook(self, input, output):\n",
    "        batch_size = input[0].size(0) if input[0].dim() == 2 else 1\n",
    "        weight_ops = self.weight.nelement() * (2 if multiply_adds else 1)\n",
    "        bias_ops = self.bias.nelement()\n",
    "        flops = batch_size * (weight_ops + bias_ops)\n",
    "        list_linear.append(flops)\n",
    "\n",
    "    list_bn=[] \n",
    "    def bn_hook(self, input, output):\n",
    "        list_bn.append(input[0].nelement())\n",
    "        \n",
    "    list_sig = []\n",
    "    def sig_hook(self, input, output):\n",
    "        list_sig.append(input[0].nelement())\n",
    "    \n",
    "    list_softmax = []\n",
    "    def softmax_hook(self, input, output):\n",
    "        print(input[0].nelement())\n",
    "        list_softmax.append(input[0].nelement())\n",
    "        \n",
    "    list_relu=[] \n",
    "    def relu_hook(self, input, output):\n",
    "        list_relu.append(input[0].nelement())\n",
    "\n",
    "    list_pooling=[]\n",
    "    def pooling_hook(self, input, output):\n",
    "        batch_size, input_channels, input_height, input_width = input[0].size()\n",
    "        output_channels, output_height, output_width = output[0].size()\n",
    "        kernel_ops = self.kernel_size * self.kernel_size\n",
    "        bias_ops = 0\n",
    "        params = output_channels * (kernel_ops + bias_ops)\n",
    "        flops = batch_size * params * output_height * output_width\n",
    "        list_pooling.append(flops)\n",
    "    \n",
    "    def foo(net):\n",
    "        childrens = list(net.children())\n",
    "        if not childrens:\n",
    "            if isinstance(net, torch.nn.Conv2d):\n",
    "                net.register_forward_hook(conv_hook)\n",
    "            if isinstance(net, torch.nn.Linear):\n",
    "                net.register_forward_hook(linear_hook)\n",
    "            if isinstance(net, torch.nn.BatchNorm2d):\n",
    "                net.register_forward_hook(bn_hook)\n",
    "            if isinstance(net, torch.nn.ReLU):\n",
    "                net.register_forward_hook(relu_hook)\n",
    "            if isinstance(net, torch.nn.MaxPool2d) or isinstance(net, torch.nn.AvgPool2d):\n",
    "                net.register_forward_hook(pooling_hook)\n",
    "            if isinstance(net, torch.nn.Sigmoid):\n",
    "                net.register_forward_hook(sig_hook)\n",
    "            if isinstance(net, torch.nn.Softmax):\n",
    "                net.register_forward_hook(softmax_hook)\n",
    "            return\n",
    "        for c in childrens:\n",
    "                foo(c)\n",
    "    foo(model)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    out = model(input.to(device))\n",
    "    total_flops = (sum(list_conv) + sum(list_linear) + sum(list_bn) + sum(list_relu) + sum(list_pooling) + sum(list_sig) +sum(list_softmax)) \n",
    "    return total_flops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    '''\n",
    "    Training and evaluation of the model.\n",
    "    '''\n",
    "    print('Training starts...')\n",
    "    for epoch in range(num_of_epoch):\n",
    "        print('\\nEpoch', epoch+1)\n",
    "        # log the start time of the epoch\n",
    "        start = time.time()\n",
    "        # set the models in training mode\n",
    "        clstm.train()\n",
    "        policy_s.train()\n",
    "        policy_n.train()\n",
    "        policy_c.train()\n",
    "        # reset the count of reread_or_skim_times\n",
    "        reread_or_skim_times = 0\n",
    "        policy_loss_sum = []\n",
    "        encoder_loss_sum = []\n",
    "        baseline_value_batch = []\n",
    "        for index, train in enumerate(train_iterator):\n",
    "            label = train.label.to(torch.long)  # for cross entropy loss, the long type is required\n",
    "            text = train.text.view(CHUNCK_SIZE, BATCH_SIZE, CHUNCK_SIZE) # transform 1*400 to 20*1*20\n",
    "            curr_step = 0  # the position of the current chunk\n",
    "            h_0 = torch.zeros([1,1,128]).to(device)  # run on GPU\n",
    "            count = 0  # maximum skim/reread time: 5\n",
    "            baseline_value_ep = []\n",
    "            saved_log_probs = []  # for the use of policy gradient update\n",
    "            # collect the computational costs for every time step\n",
    "            cost_ep = []  \n",
    "            while curr_step < CHUNCK_SIZE and count < 5: \n",
    "                # Loop until a text can be classified or currstep is up to 20 or count reach the maximum i.e. 5.\n",
    "                # update count\n",
    "                count += 1\n",
    "                # pass the input through cnn-lstm and policy s\n",
    "                text_input = text[curr_step] # text_input 1*20\n",
    "                ht = clstm(text_input, h_0)  # 1 * 128\n",
    "                # separate the value which is the input of value net\n",
    "                ht_ = ht.clone().detach().requires_grad_(True)\n",
    "                # compute a baseline value for the value network\n",
    "                bi = value_net(ht_)\n",
    "                # 1 * 1 * 128, next input of lstm\n",
    "                h_0 = ht.unsqueeze(0)\n",
    "                # draw a stop decision\n",
    "                stop_decision, log_prob_s = sample_policy_s(ht, policy_s)\n",
    "                stop_decision = stop_decision.item()\n",
    "                if stop_decision == 1: # classify\n",
    "                    break\n",
    "                else: \n",
    "                    reread_or_skim_times += 1\n",
    "                    # draw an action (reread or skip)\n",
    "                    step, log_prob_n = sample_policy_n(ht, policy_n)\n",
    "                    curr_step += int(step)  # reread or skip\n",
    "                    if curr_step < CHUNCK_SIZE and count < 5:\n",
    "                        # If the code can still execute the next loop, it is not the last time step.\n",
    "                        cost_ep.append(clstm_cost + s_cost + n_cost)\n",
    "                        # add the baseline value\n",
    "                        baseline_value_ep.append(bi)\n",
    "                        # add the log prob for the current actions\n",
    "                        saved_log_probs.append(log_prob_s + log_prob_n)\n",
    "            # draw a predicted label\n",
    "            output_c = policy_c(ht)\n",
    "            # cross entrpy loss input shape: input(N, C), target(N)\n",
    "            loss = criterion(output_c, label)  # positive value\n",
    "            # draw a predicted label \n",
    "            pred_label, log_prob_c = sample_policy_c(output_c)\n",
    "            if stop_decision == 1:\n",
    "                # add the cost of the last time step\n",
    "                cost_ep.append(clstm_cost + s_cost + c_cost)\n",
    "                saved_log_probs.append(log_prob_s + log_prob_c)\n",
    "            else:\n",
    "                # add the cost of the last time step\n",
    "                cost_ep.append(clstm_cost + s_cost + c_cost + n_cost)\n",
    "                # At the moment, the probability of drawing a stop decision is 1,\n",
    "                # so its log probability is zero which can be ignored in th sum.\n",
    "                saved_log_probs.append(log_prob_c.unsqueeze(0))\n",
    "            # add the baseline value\n",
    "            baseline_value_ep.append(bi)\n",
    "            # add the cross entropy loss\n",
    "            encoder_loss_sum.append(loss)\n",
    "            # compute the policy losses and value losses for the current episode\n",
    "            policy_loss_ep, value_losses = compute_policy_value_losses(cost_ep, loss, saved_log_probs, baseline_value_ep, alpha, gamma)\n",
    "            policy_loss_sum.append(torch.cat(policy_loss_ep).sum())\n",
    "            baseline_value_batch.append(torch.cat(value_losses).sum())\n",
    "            # update gradients\n",
    "            if (index + 1) % batch_sz == 0:  # take the average of 50 samples\n",
    "                finish_episode(policy_loss_sum, encoder_loss_sum, baseline_value_batch)\n",
    "                del policy_loss_sum[:], encoder_loss_sum[:], baseline_value_batch[:]\n",
    "                \n",
    "            if (index + 1) % 2000 == 0:\n",
    "                print('\\n current episode: ',{index + 1})\n",
    "                # log the current position of the text which the agent has gone through\n",
    "                print('curr_step: ', curr_step)\n",
    "                # log the sum of the rereading and skimming times\n",
    "                print('current reread_or_skim_times: ',{reread_or_skim_times})\n",
    "\n",
    "\n",
    "        print('Epoch time elapsed: %.2f s' % (time.time() - start))\n",
    "        print('reread_or_skim_times in this epoch:', reread_or_skim_times)\n",
    "        count_all, count_correct = evaluate(clstm, policy_s, policy_n, policy_c, valid_iterator)\n",
    "        print('Epoch: %s, Accuracy on the validation set: %.2f' % (epoch + 1, count_correct / count_all))\n",
    "        count_all, count_correct = evaluate(clstm, policy_s, policy_n, policy_c, train_iterator)\n",
    "        print('Epoch: %s, Accuracy on the training set: %.2f' % (epoch + 1, count_correct / count_all))\n",
    "        \n",
    "    print('Compute the accuracy on the testing set...')\n",
    "    count_all, count_correct = evaluate(clstm, policy_s, policy_n, policy_c, test_iterator)\n",
    "    print('Accuracy on the testing set: %.2f' % (count_correct / count_all))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
